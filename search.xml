<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>从零开始在CentOS7服务器上搭建GPU深度学习环境，并部署bert向量化服务</title>
    <url>/2021/01/04/build-gpu-deep-learning-environment-and-deploy-bert-service-on-centos7/</url>
    <content><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>这篇文档将以centos7服务器为例，介绍搭建GPU深度学习环境，并部署bert向量化服务<a href="https://github.com/hanxiao/bert-as-service">bert-as-service</a>的步骤，以及搭建过程中遇到的一些问题和解决方法，给后续在其他机器上的部署提供参考。</p>
<span id="more"></span>
<p>最近公司的研究项目有对文本进行BERT向量化的需求，在github上找到了<a href="https://github.com/hanxiao/bert-as-service">bert-as-service</a>这个开源项目，它提供pip安装，可以将bert向量化作为服务部署在服务器上，在客户端python脚本中通过简单的调用请求就可以得到词向量。相比自己调用bert源码，这种方法方便非常多。可以参考这篇知乎文章：<a href="https://zhuanlan.zhihu.com/p/50582974">两行代码玩转Google BERT句向量词向量</a></p>
<p>搭建完成后，生产力有了质的飞跃：在本地的cpu机器上，处理一段中文短文本大约需要1s，而单卡gpu服务器上只需要30多秒就可以处理1000段短文本。</p>
<p>注意：</p>
<ul>
<li>一定要先切换到一个临时目录，因为bert-service会产生很多临时文件在当前文件夹下</li>
<li>需要开放5555、5556端口（默认）</li>
</ul>
<h2 id="1-GPU环境搭建"><a href="#1-GPU环境搭建" class="headerlink" title="1 GPU环境搭建"></a>1 GPU环境搭建</h2><h3 id="1-1-准备工作"><a href="#1-1-准备工作" class="headerlink" title="1.1 准备工作"></a>1.1 准备工作</h3><p>查看系统位数<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ uname -a</span><br><span class="line">Linux node14 3.10.0-1127.el7.x86_64 #1 SMP Tue Mar 31 23:36:51 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux</span><br></pre></td></tr></table></figure><br>可以看到，服务器是x86_64架构，接下来的包都可以安装。</p>
<p>查看linux发行版本（以centOS为例）<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cat /etc/centos-release</span><br><span class="line">CentOS Linux release 7.8.2003 (Core)</span><br></pre></td></tr></table></figure><br>实际上，服务器深度学习环境更多使用的是Ubuntu。本文从实际情况出发，介绍在CentOS 7.x上的部署。</p>
<p>查看显卡信息<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ lspci  | grep -i vga</span><br><span class="line">01:00.0 VGA compatible controller: NVIDIA Corporation TU102 [GeForce RTX 2080 Ti Rev. A] (rev a1)</span><br></pre></td></tr></table></figure><br><strong><em>注意</em></strong>，以下的环境搭建基于NVIDIA显卡。其他品牌的显卡虽然有其他框架可以使用，但还不成熟。</p>
<p>可以看到，214服务器的显卡为2080Ti，是不错的显卡<del>不拿来做深度学习可惜了</del>，但是服务器通常只是在跑一些内存任务，显卡没有得到充分利用。</p>
<h3 id="1-2-安装Anaconda-miniconda"><a href="#1-2-安装Anaconda-miniconda" class="headerlink" title="1.2 安装Anaconda/miniconda"></a>1.2 安装Anaconda/miniconda</h3><p>安装Anaconda/miniconda用于管理环境。这一步通常不会有什么问题。考虑到服务器上不需要装那么多乱七八糟的包，安装了miniconda3。</p>
<ul>
<li><a href="https://docs.conda.io/en/latest/miniconda.html">miniconda下载</a></li>
<li><a href="https://conda.io/projects/conda/en/latest/user-guide/install/linux.html">miniconda安装文档-linux</a></li>
</ul>
<p><em>安装conda的一个好处是，在activate/deactivate某个虚拟环境的时候，可以执行特定的脚本，这将有助于我们在不同环境中使用不同CUDA版本，而不需要每次都手动修改环境变量，这将在下文详细介绍。参考：<a href="https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77">Multiple Version of CUDA Libraries On The Same Machine</a></em></p>
<h3 id="1-3-安装CUDA和cuDNN"><a href="#1-3-安装CUDA和cuDNN" class="headerlink" title="1.3 安装CUDA和cuDNN"></a>1.3 安装CUDA和cuDNN</h3><p>务必<strong>不要直接安装最新版CUDA工具包</strong>，这是个巨坑。tensorflow和pytorch的不同版本都对应不同的CUDA版本，假如CUDA版本不匹配，可能会出现无法正常使用GPU进行计算的情况。（这就是我最初部署bert-service时，显示worker是GPU，但实际仍然在CPU上进行计算的原因。服务可以正常跑起来，处理请求的时候CPU直接拉满。）</p>
<ul>
<li><a href="https://developer.nvidia.com/cuda-toolkit-archive">各版本CUDA下载</a></li>
<li><a href="https://developer.nvidia.com/rdp/cudnn-download">cuDNN下载</a>，注意和CUDA版本的对应（cuDNN安装较新的版本可以覆盖旧的版本，没有报错），下载需要注册NVIDIA developer。</li>
<li><a href="https://docs.nvidia.com/cuda/archive/11.0/cuda-installation-guide-linux/index.html">CUDA 11.0安装文档</a></li>
<li><a href="https://docs.nvidia.com/cuda/archive/10.0/cuda-installation-guide-linux/index.html">CUDA 10.0安装文档</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#troubleshoot">cuDNN 8.0.5安装文档</a></li>
</ul>
<p>请务必参照tensorflow、pytorch官方文档，选择合适的CUDA版本进行安装。</p>
<ul>
<li>tensorflow-gpu版本和CUDA版本的对应：<a href="https://www.tensorflow.org/install/source#common_installation_problems">官方文档</a></li>
<li><a href="https://pytorch.org">pytorch官网</a>，注意不同CUDA版本对应的安装命令有所不同！选择合适的版本，会有对应的conda/pip安装命令</li>
</ul>
<p>CUDA可以同时安装多个版本，使用时根据需要配置环境变量，例如214服务器上就安装了10.0和11.0两个版本。安装多版本CUDA<strong>建议使用runfile安装</strong>，安装选项选择不安装Driver（因为系统里已经有Driver，即显卡驱动了）。使用rpm安装可能因为已安装相同的包而产生冲突。</p>
<p>以最终部署在214服务器上的版本为例，tensorflow-gpu 1.14.0版本对应CUDA 10.0。cuDNN的版本只需要7.4以上即可。最终我安装的是CUDA 11.0对应的cuDNN 8版本，运行没有问题。</p>
<p>CUDA的默认安装路径为<code>/usr/local/cuda-xx.x/</code>，其中<code>xx.x</code>为版本号。<strong>注意</strong>CUDA安装完成后必须手动配置环境变量，切换CUDA版本也只需要更改环境变量，比如<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ export PATH=/usr/local/cuda-11.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">$ export LD_LIBRARY_PATH=/usr/local/cuda-11.0/lib64\</span><br><span class="line">    $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><br>和<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ export PATH=/usr/local/cuda-10.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">$ export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64\</span><br><span class="line">    $&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125;</span><br></pre></td></tr></table></figure><br>就实现了CUDA 11.0和10.0版本的切换。</p>
<p>要查看当前的runtime CUDA版本可以通过<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ nvcc -V</span><br><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2020 NVIDIA Corporation</span><br><span class="line">Built on Thu_Jun_11_22:26:38_PDT_2020</span><br><span class="line">Cuda compilation tools, release 11.0, V11.0.194</span><br><span class="line">Build cuda_11.0_bu.TC445_37.28540450_0</span><br></pre></td></tr></table></figure></p>
<h2 id="2-【非常重要】确保tensorflow可以正常使用GPU"><a href="#2-【非常重要】确保tensorflow可以正常使用GPU" class="headerlink" title="2 【非常重要】确保tensorflow可以正常使用GPU"></a>2 【非常重要】确保tensorflow可以正常使用GPU</h2><p>在python中测试tensorflow、pytorch是否可以使用GPU：<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; tf.test.is_gpu_available()</span><br><span class="line">&gt;&gt;&gt; import torch</span><br><span class="line">&gt;&gt;&gt; torch.cuda.is_available()</span><br></pre></td></tr></table></figure></p>
<p>这里以tensorflow为例。</p>
<p>假如不能正常使用GPU，<code>tf.test.is_gpu_available()</code>会显示报错信息，比如，在CUDA 10.0环境下，tensorflow1.14.0<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">&gt;&gt;&gt; tf.test.is_gpu_available()</span><br><span class="line">......</span><br><span class="line">2020-12-01 10:26:58.163460: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library &#x27;libcudnn.so.7&#x27;; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-11.0/lib64</span><br><span class="line">2020-12-01 10:26:58.163474: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...</span><br><span class="line">2020-12-01 10:26:58.163490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:</span><br><span class="line">2020-12-01 10:26:58.163498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 </span><br><span class="line">2020-12-01 10:26:58.163510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N </span><br><span class="line">False</span><br></pre></td></tr></table></figure><br>因为找不到<code>libcudnn.so.7</code>文件，所以GPU不可用。通过创建一个软链接，可以解决这个问题。首先，找到libcudnn的安装位置，以rpm安装的cudnn为例（网上有通过tgz安装的解决方法，然而我在官网只看到了rpm安装包）<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ rpm -ql libcudnn8</span><br><span class="line">/usr/lib64/libcudnn.so.8</span><br><span class="line">/usr/lib64/libcudnn.so.8.0.5</span><br><span class="line">/usr/lib64/libcudnn_adv_infer.so.8</span><br><span class="line">/usr/lib64/libcudnn_adv_infer.so.8.0.5</span><br><span class="line">/usr/lib64/libcudnn_adv_train.so.8</span><br><span class="line">/usr/lib64/libcudnn_adv_train.so.8.0.5</span><br><span class="line">/usr/lib64/libcudnn_cnn_infer.so.8</span><br><span class="line">/usr/lib64/libcudnn_cnn_infer.so.8.0.5</span><br><span class="line">/usr/lib64/libcudnn_cnn_train.so.8</span><br><span class="line">/usr/lib64/libcudnn_cnn_train.so.8.0.5</span><br><span class="line">/usr/lib64/libcudnn_ops_infer.so.8</span><br><span class="line">/usr/lib64/libcudnn_ops_infer.so.8.0.5</span><br><span class="line">/usr/lib64/libcudnn_ops_train.so.8</span><br><span class="line">/usr/lib64/libcudnn_ops_train.so.8.0.5</span><br></pre></td></tr></table></figure></p>
<p>然后，创建软链接<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ln -s /usr/lib64/libcudnn.so /usr/local/cuda-10.0/lib64/libcudnn.so.7</span><br></pre></td></tr></table></figure><br>再次测试，返回结果为True。</p>
<p>又比如，在CUDA 11.0环境下，tensorflow-gpu 2.3.1，import提示<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">2020-12-01 16:35:49.357365: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library &#x27;libcudart.so.10.1&#x27;; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64</span><br><span class="line">2020-12-01 16:35:49.357395: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.</span><br></pre></td></tr></table></figure><br>创建软链接<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ln -s /usr/local/cuda-11.0/lib64/libcudart.so /usr/local/cuda-11.0/lib64/libcudart.so.10.1</span><br></pre></td></tr></table></figure><br>再次尝试<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">2020-12-01 16:40:30.623510: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1</span><br><span class="line">&gt;&gt;&gt; tf.test.is_gpu_available()</span><br><span class="line">......</span><br><span class="line">2020-12-01 16:41:20.527629: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library &#x27;libcusparse.so.10&#x27;; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64</span><br><span class="line">2020-12-01 16:41:20.527687: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library &#x27;libcudnn.so.7&#x27;; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64</span><br><span class="line">......</span><br><span class="line">False</span><br></pre></td></tr></table></figure><br>创建软链接<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ ln -s /usr/local/cuda-11.0/lib64/libcublas.so /usr/local/cuda-11.0/lib64/libcublas.so.10</span><br><span class="line">$ ln -s /usr/lib64/libcudnn.so /usr/local/cuda-11.0/lib64/libcudnn.so.7</span><br><span class="line">$ ln -s /usr/local/cuda-11.0/lib64/libcusparse.so /usr/local/cuda-11.0/lib64/libcusparse.so.10</span><br></pre></td></tr></table></figure><br>再次测试<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorflow as tf</span><br><span class="line">2020-12-01 16:40:30.623510: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1</span><br><span class="line">&gt;&gt;&gt; tf.test.is_gpu_available()</span><br><span class="line">......</span><br><span class="line">True</span><br></pre></td></tr></table></figure><br><strong>总而言之，缺什么<code>libxxxx</code>只需要在缺少文件的路径下创建一个软链接到（更高版本的）同名文件。</strong></p>
<p>参考：</p>
<ul>
<li><a href="https://blog.csdn.net/weixin_40298200/article/details/79420758">【UBUNTU深度学习环境】ImportError: libcudnn.so.7: cannot open shared object file: No such file or directory</a></li>
<li><a href="https://blog.csdn.net/qq_27481295/article/details/102799977">【pytorch】libcudart.so.10.1: cannot open shared object file: No such file or directory</a></li>
</ul>
<h2 id="3-bert-as-service的部署与使用"><a href="#3-bert-as-service的部署与使用" class="headerlink" title="3 bert-as-service的部署与使用"></a>3 bert-as-service的部署与使用</h2><h3 id="3-1-conda创建虚拟环境"><a href="#3-1-conda创建虚拟环境" class="headerlink" title="3.1 conda创建虚拟环境"></a>3.1 conda创建虚拟环境</h3><h4 id="3-1-1-创建python-3-6虚拟环境"><a href="#3-1-1-创建python-3-6虚拟环境" class="headerlink" title="3.1.1 创建python 3.6虚拟环境"></a>3.1.1 创建python 3.6虚拟环境</h4><p>（python 3.7中tensorflow 1.14.0或之前版本import会报错），激活环境。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ conda create -n bertservice -python=3.6</span><br><span class="line">$ conda activate bertservice</span><br></pre></td></tr></table></figure></p>
<h4 id="3-1-2-【可选】设定pip国内镜像"><a href="#3-1-2-【可选】设定pip国内镜像" class="headerlink" title="3.1.2 【可选】设定pip国内镜像"></a>3.1.2 【可选】设定pip国内镜像</h4><p>比如豆瓣。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip config set global.index-url https://pypi.douban.com/simple</span><br></pre></td></tr></table></figure></p>
<h4 id="3-1-3-安装tensorflow-gpu-1-x"><a href="#3-1-3-安装tensorflow-gpu-1-x" class="headerlink" title="3.1.3 安装tensorflow-gpu 1.x"></a>3.1.3 安装tensorflow-gpu 1.x</h4><p>（因为2.x版本无法正常启动bert-service）。<strong><em>注意</em></strong>，tensorflow一定一定要安装<strong>gpu版本</strong>，假如已经装了cpu版本，请卸载之，否则就算装了gpu版，import的还是cpu版。<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip install tensorflow-gpu==1.14.0</span><br></pre></td></tr></table></figure></p>
<h4 id="3-1-4-服务端只需要安装bert-serving-server包。"><a href="#3-1-4-服务端只需要安装bert-serving-server包。" class="headerlink" title="3.1.4 服务端只需要安装bert-serving-server包。"></a>3.1.4 服务端只需要安装bert-serving-server包。</h4><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip install bert-serving-server</span><br></pre></td></tr></table></figure>
<h4 id="3-1-5-配置activate-deactivate虚拟环境时自动修改环境变量的脚本"><a href="#3-1-5-配置activate-deactivate虚拟环境时自动修改环境变量的脚本" class="headerlink" title="3.1.5 配置activate/deactivate虚拟环境时自动修改环境变量的脚本"></a>3.1.5 配置activate/deactivate虚拟环境时自动修改环境变量的脚本</h4><p>这样，CUDA版本切换后无需手动配置环境变量。</p>
<p>参考：</p>
<ul>
<li><a href="https://blog.kovalevskyi.com/multiple-version-of-cuda-libraries-on-the-same-machine-b9502d50ae77">Multiple Version of CUDA Libraries On The Same Machine</a></li>
<li><a href="https://blog.csdn.net/hizengbiao/article/details/88625044">非root用户在linux下安装多个版本的CUDA和cuDNN（cuda 8、cuda 10.1 等）</a></li>
</ul>
<p>操作步骤：</p>
<ol>
<li><p>cd到conda创建的环境目录下。例如</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ cd ~/miniconda3/envs/bertservice</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建目录和脚本文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ mkdir ./etc/conda/activate.d</span><br><span class="line">$ mkdir ./etc/conda/deactivate.d</span><br><span class="line">$ touch ./etc/conda/activate.d/activate.sh</span><br><span class="line">$ touch ./etc/conda/deactivate.d/deactivate.sh</span><br></pre></td></tr></table></figure>
<p>脚本文件内容如下：</p>
</li>
</ol>
<p><code>activate.sh</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">export PATH=/usr/local/cuda-10.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">ORIGINAL_LD_LIBRARY_PATH=$LD_LIBRARY_PATH</span><br><span class="line">export LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64</span><br></pre></td></tr></table></figure><br><code>deactivate.sh</code><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/sh</span><br><span class="line">export PATH=/usr/local/cuda-11.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;</span><br><span class="line">export LD_LIBRARY_PATH=$ORIGINAL_LD_LIBRARY_PATH</span><br><span class="line">unset ORIGINAL_LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><br>这里，11.0是<code>base</code>环境的CUDA版本，10.0是<code>bertservice</code>环境的CUDA版本。</p>
<h3 id="3-2-服务端启动服务"><a href="#3-2-服务端启动服务" class="headerlink" title="3.2 服务端启动服务"></a>3.2 服务端启动服务</h3><p>启动服务，需要指定模型路径，提前下载bert参数文件，我们需要用到的是中文的<a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip">BERT-Base, Chinese</a>。关于api的详细说明请参考<a href="https://github.com/hanxiao/bert-as-service">bert-service的github官方文档</a>。</p>
<p>启动命令例：模型路径、worker数量、向客户端返回token（实际情况下不需要，因为中文bert是字级别的）、不限制句子长度（最大长度是512，超过的部分将会被截断）<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ bert-serving-start -model_dir ~/models/chinese_L-12_H-768_A-12/ -num_worker=1 -show_tokens_to_client -max_seq_len=None</span><br></pre></td></tr></table></figure></p>
<h3 id="3-3-客户端调用"><a href="#3-3-客户端调用" class="headerlink" title="3.3 客户端调用"></a>3.3 客户端调用</h3><p>客户端只需要安装bert-serving-client包<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip install bert-serving-client</span><br></pre></td></tr></table></figure><br>然后在客户端python脚本中调用<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; from bert_serving.client import BertClient</span><br><span class="line">&gt;&gt;&gt; bc = BertClient(ip=&#x27;localhost&#x27;) #服务器ip</span><br><span class="line">&gt;&gt;&gt; out = bc.encode([&quot;其实从２０１１下半年开始，中国风就在表坛成为当仁不让的话题。&quot;],show_tokens=False, is_tokenized=False)</span><br></pre></td></tr></table></figure><br><strong><em>注意</em></strong>，<code>bc.encode()</code>接收的参数类型是list。详细api请同样参考<a href="https://github.com/hanxiao/bert-as-service">文档</a>。</p>
<h2 id="4-遇到的其他问题"><a href="#4-遇到的其他问题" class="headerlink" title="4 遇到的其他问题"></a>4 遇到的其他问题</h2><p>以下的问题，按照上述安装方法，应该不会遇到。</p>
<h3 id="4-1-卸载CUDA的时候不小心把显卡驱动（NVIDIA-Driver）一并卸载了"><a href="#4-1-卸载CUDA的时候不小心把显卡驱动（NVIDIA-Driver）一并卸载了" class="headerlink" title="4.1 卸载CUDA的时候不小心把显卡驱动（NVIDIA Driver）一并卸载了"></a>4.1 卸载CUDA的时候不小心把显卡驱动（NVIDIA Driver）一并卸载了</h3><p>重装显卡驱动：上<a href="https://www.nvidia.cn/Download/index.aspx">官网</a>找到显卡对应的驱动，下载，重装。安装完成后，可以正常执行<code>nvidia-smi</code>命令，查看显卡情况，不需要reboot。<del>服务器还在执行别的任务，不能重启，差点以为把服务器玩坏了。</del><br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ nvidia-smi</span><br><span class="line">Tue Dec  1 15:05:55 2020       </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0     |</span><br><span class="line">|-------------------------------+----------------------+----------------------+</span><br><span class="line">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |</span><br><span class="line">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |</span><br><span class="line">|                               |                      |               MIG M. |</span><br><span class="line">|===============================+======================+======================|</span><br><span class="line">|   0  GeForce RTX 208...  Off  | 00000000:01:00.0 Off |                  N/A |</span><br><span class="line">| 12%   58C    P0    41W / 250W |      0MiB / 11011MiB |      0%      Default |</span><br><span class="line">|                               |                      |                  N/A |</span><br><span class="line">+-------------------------------+----------------------+----------------------+</span><br><span class="line">                                                                               </span><br><span class="line">+-----------------------------------------------------------------------------+</span><br><span class="line">| Processes:                                                                  |</span><br><span class="line">|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |</span><br><span class="line">|        ID   ID                                                   Usage      |</span><br><span class="line">|=============================================================================|</span><br><span class="line">|  No running processes found                                                 |</span><br><span class="line">+-----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure></p>
<h3 id="4-2-nvidia-smi与nvcc-V显示的CUDA版本不一致"><a href="#4-2-nvidia-smi与nvcc-V显示的CUDA版本不一致" class="headerlink" title="4.2 nvidia-smi与nvcc -V显示的CUDA版本不一致"></a>4.2 nvidia-smi与nvcc -V显示的CUDA版本不一致</h3><p>没毛病，<code>nvidia-smi</code>是Driver版本，<code>nvcc -V</code>是runtime版本。参考：<a href="https://blog.csdn.net/ljp1919/article/details/102640512">nvidia-smi 和 nvcc 结果的版本为何不一致</a></p>
<h3 id="4-3-尝试使用docker中的tensorflow-gpu镜像"><a href="#4-3-尝试使用docker中的tensorflow-gpu镜像" class="headerlink" title="4.3 尝试使用docker中的tensorflow-gpu镜像"></a>4.3 尝试使用docker中的tensorflow-gpu镜像</h3><p>按照官方文档操作，docker并没有运行成功。最后放弃了该方法。</p>
<p>参考：</p>
<ul>
<li><a href="https://www.tensorflow.org/install/docker">https://www.tensorflow.org/install/docker</a></li>
<li><a href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#setting-up-docker-on-rhel-7">https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#setting-up-docker-on-rhel-7</a></li>
</ul>
<hr>
<p>This is an archived post. Originally posted on <a href="https://blog.csdn.net/weixin_43538536/article/details/112193553">CSDN</a>.</p>
]]></content>
      <tags>
        <tag>linux</tag>
        <tag>python</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title>Create Personal Website with Hexo and Github Pages</title>
    <url>/2022/10/25/create-personal-website-with-hexo-and-github-pages/</url>
    <content><![CDATA[<p>In this post, I will explain the details I build my own Github homepage with <a href="https://hexo.io">Hexo</a>, theme <a href="https://github.com/theme-next/hexo-theme-next">NexT</a>,  and <a href="https://pages.github.com">Github Pages</a>.</p>
<p>Basically, I followed the steps in this Zhihu answer:<br><a href="https://www.zhihu.com/question/20962496/answer/1882882782">https://www.zhihu.com/question/20962496/answer/1882882782</a></p>
<span id="more"></span>
<h2 id="Install-Hexo"><a href="#Install-Hexo" class="headerlink" title="Install Hexo"></a>Install Hexo</h2><p>Requirements:</p>
<ul>
<li><a href="https://nodejs.org/en/">Node.js</a> (recommends 12.0 or higher)</li>
<li><a href="https://git-scm.com">Git</a></li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install -g hexo-cli</span><br></pre></td></tr></table></figure>
<p>Refer to: <a href="https://hexo.io/docs/">https://hexo.io/docs/</a></p>
<h2 id="Hexo-Quick-Start"><a href="#Hexo-Quick-Start" class="headerlink" title="Hexo Quick Start"></a>Hexo Quick Start</h2><h3 id="Initialize-a-Hexo-project"><a href="#Initialize-a-Hexo-project" class="headerlink" title="Initialize a Hexo project"></a>Initialize a Hexo project</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt;</span><br><span class="line">$ <span class="built_in">cd</span> &lt;folder&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure>
<p>Refer to: <a href="https://hexo.io/docs/setup">https://hexo.io/docs/setup</a></p>
<p>In my case, I want to use this Hexo project as my Github homepage:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;your-username.github.io&gt;</span><br><span class="line">$ <span class="built_in">cd</span> &lt;your-username.github.io&gt;</span><br><span class="line">$ npm install</span><br></pre></td></tr></table></figure></p>
<h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<p>Remember to perform <code>hexo clean</code> and <code>hexo generate</code> after you modified your site.<br>Then proceed to <code>hexo server</code> for local test or <code>hexo depoly</code> for deployment.</p>
<h3 id="Run-server-locally"><a href="#Run-server-locally" class="headerlink" title="Run server locally"></a>Run server locally</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<p>This runs Hexo locally, so that you can view your site before deploying it to remote sites.<br>The output is like:<br><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">INFO  Validating config</span><br><span class="line">INFO  Start processing</span><br><span class="line">INFO  Hexo is running at http://localhost:4000/ . Press Ctrl+C to stop.</span><br></pre></td></tr></table></figure></p>
<h3 id="Change-Hexo-config"><a href="#Change-Hexo-config" class="headerlink" title="Change Hexo config"></a>Change Hexo config</h3><p>Modify <code>_config.yml</code> in the site root directory.<br>This changes global settings for Hexo.</p>
<p>Please refer to <a href="https://hexo.io/docs/configuration">Configuration | Hexo</a> for details.</p>
<h3 id="Change-Hexo-theme"><a href="#Change-Hexo-theme" class="headerlink" title="Change Hexo theme"></a>Change Hexo theme</h3><h4 id="NexT"><a href="#NexT" class="headerlink" title="NexT"></a>NexT</h4><p>I use theme <a href="https://github.com/theme-next/hexo-theme-next">NexT</a> for my site:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> &lt;your-hexo-site&gt;</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/theme-next/hexo-theme-next themes/next</span><br></pre></td></tr></table></figure></p>
<p>Modify <code>_config.yml</code> in the site root directory to enable the theme:<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">theme:</span> <span class="string">next</span></span><br></pre></td></tr></table></figure></p>
<p>Modify <code>theme/next/_config.yml</code> for theme-specific config.</p>
<p>Please refer to <a href="https://theme-next.js.org">NexT Documentation</a> for details.</p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
<h4 id="Github-Pages"><a href="#Github-Pages" class="headerlink" title="Github Pages"></a>Github Pages</h4><p>Install plugin:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></p>
<p>Modify <code>_config.yml</code> in site root dir:<br><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></table></figure></p>
<p>Then run <code>hexo deploy</code>.<br>You should be able to see you site at <code>&lt;username&gt;.github.io</code>.</p>
<h2 id="Optional-Save-Site-Source-Code-on-Github"><a href="#Optional-Save-Site-Source-Code-on-Github" class="headerlink" title="(Optional) Save Site Source Code on Github"></a>(Optional) Save Site Source Code on Github</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git init</span><br><span class="line">$ git checkout -b <span class="built_in">source</span></span><br><span class="line">$ git add -A</span><br><span class="line">$ git commit -m <span class="string">&quot;init blog&quot;</span></span><br><span class="line">$ git remote add origin git@github.com:&#123;username&#125;/&#123;username&#125;.github.io.git</span><br><span class="line">$ git push origin <span class="built_in">source</span></span><br></pre></td></tr></table></figure>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Note that some of them are written in Chinese.</p>
<p>Documentation</p>
<ul>
<li><a href="https://hexo.io/zh-cn/docs/">文档 | Hexo</a></li>
<li><a href="https://hexo.io/docs/">Documentation | Hexo</a></li>
<li><a href="https://theme-next.js.org">NexT Documentation</a></li>
<li><a href="http://theme-next.iissnan.com">NexT使用文档</a></li>
</ul>
<p>Online Posts</p>
<ul>
<li><a href="https://www.zhihu.com/question/20962496/answer/1882882782">https://www.zhihu.com/question/20962496/answer/1882882782</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/149306963">https://zhuanlan.zhihu.com/p/149306963</a></li>
<li><a href="https://uchuhimo.me/2017/04/11/genesis">https://uchuhimo.me/2017/04/11/genesis</a></li>
<li><a href="https://www.jianshu.com/p/53670692c5a6">https://www.jianshu.com/p/53670692c5a6</a></li>
<li><a href="https://www.jianshu.com/p/a9e0b95f57a5">https://www.jianshu.com/p/a9e0b95f57a5</a></li>
</ul>
]]></content>
      <tags>
        <tag>github</tag>
        <tag>website</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title>httpd重启失败No space left on device: AH00023: Couldn&#39;t create the ssl-cache mutex</title>
    <url>/2020/12/25/httpd-service-start-failed/</url>
    <content><![CDATA[<p>httpd.service重启失败，报错信息如下：</p>
<span id="more"></span>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># service httpd start</span><br><span class="line">Redirecting to /bin/systemctl start httpd.service</span><br><span class="line">Job for httpd.service failed because the control process exited with error code. See &quot;systemctl status httpd.service&quot; and &quot;journalctl -xe&quot; for details.</span><br><span class="line"># service httpd status</span><br><span class="line">Redirecting to /bin/systemctl status httpd.service</span><br><span class="line">● httpd.service - The Apache HTTP Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: failed (Result: exit-code) since Tue 2020-11-24 10:23:57 CST; 5s ago</span><br><span class="line">     Docs: man:httpd(8)</span><br><span class="line">           man:apachectl(8)</span><br><span class="line">  Process: 16430 ExecReload=/usr/sbin/httpd $OPTIONS -k graceful (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 14493 ExecStart=/usr/sbin/httpd $OPTIONS -DFOREGROUND (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 14493 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Nov 24 10:23:57 izuf68hjhpsqp9dl4p2c9cz systemd[1]: Starting The Apache HTTP Server...</span><br><span class="line">Nov 24 10:23:57 izuf68hjhpsqp9dl4p2c9cz systemd[1]: httpd.service: main process exited, code=exited, status=1/FAILURE</span><br><span class="line">Nov 24 10:23:57 izuf68hjhpsqp9dl4p2c9cz systemd[1]: Failed to start The Apache HTTP Server.</span><br><span class="line">Nov 24 10:23:57 izuf68hjhpsqp9dl4p2c9cz systemd[1]: Unit httpd.service entered failed state.</span><br><span class="line">Nov 24 10:23:57 izuf68hjhpsqp9dl4p2c9cz systemd[1]: httpd.service failed.</span><br></pre></td></tr></table></figure>
<p>查看错误日志：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># tail -f  /var/log/httpd/error_log</span><br><span class="line">[Tue Nov 24 10:25:01.344079 2020] [suexec:notice] [pid 14748] AH01232: suEXEC mechanism enabled (wrapper: /usr/sbin/suexec)</span><br><span class="line">[Tue Nov 24 10:25:01.344489 2020] [core:emerg] [pid 14748] (28)No space left on device: AH00023: Couldn&#x27;t create the ssl-cache mutex </span><br><span class="line">AH00016: Configuration Failed</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>ipcs命令，查看apache分析消息队列、共享内存和信号量：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ipcs -s | grep apache</span><br><span class="line">0x00000000 163840     apache     600        1         </span><br><span class="line">0x00000000 98305      apache     600        1         </span><br><span class="line">0x00000000 2          apache     600        1         </span><br><span class="line">0x00000000 98307      apache     600        1         </span><br><span class="line">0x00000000 98308      apache     600        1         </span><br><span class="line">0x00000000 98309      apache     600        1         </span><br><span class="line">0x00000000 163846     apache     600        1         </span><br><span class="line">0x00000000 163847     apache     600        1         </span><br><span class="line">0x00000000 8          apache     600        1         </span><br><span class="line">0x00000000 9          apache     600        1</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>删除apache分析消息队列、共享内存和信号量：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ipcs -s | grep apache | perl -e &#x27;while (&lt;STDIN&gt;) &#123; @a=split(/\s+/); print `ipcrm sem $a[1]`&#125;&#x27;</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">resource(s) deleted</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
<p>重启httpd，成功！</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># service httpd restart</span><br><span class="line">Redirecting to /bin/systemctl restart httpd.service</span><br><span class="line"># service httpd status</span><br><span class="line">Redirecting to /bin/systemctl status httpd.service</span><br><span class="line">● httpd.service - The Apache HTTP Server</span><br><span class="line">   Loaded: loaded (/usr/lib/systemd/system/httpd.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: active (running) since Tue 2020-11-24 10:33:02 CST; 6s ago</span><br><span class="line">     Docs: man:httpd(8)</span><br><span class="line">           man:apachectl(8)</span><br><span class="line">  Process: 16430 ExecReload=/usr/sbin/httpd $OPTIONS -k graceful (code=exited, status=0/SUCCESS)</span><br><span class="line"> Main PID: 15687 (/usr/sbin/httpd)</span><br></pre></td></tr></table></figure>
<p>参考文章：</p>
<p><a href="https://www.cnblogs.com/syy714363310/p/12202535.html">Apache排查问题：Apache ERROR: No space left on device: AH00023: Couldn’t create the ssl-cache mutex</a></p>
<p><a href="https://www.cnblogs.com/wangkangluo1/archive/2012/06/04/2535042.html">ipcs 命令</a></p>
<hr>
<p>This is an archived post. Originally posted on <a href="https://blog.csdn.net/weixin_43538536/article/details/111662494">CSDN</a>.</p>
]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu 24.04 LTS Upgrade Kernel to Enable Wi-Fi 7</title>
    <url>/2024/09/17/ubuntu-24-04-enable-wifi-7/</url>
    <content><![CDATA[<p>I recently upgraded the OS of my PC from Ubuntu 22.04 LTS to 24.04 LTS. </p>
<p>I want to enable Wi-Fi 7, which works perfectly on the same device with Windows 11,<br>but only shows Bluetooth on Ubuntu (both 22.04 LTS and 24.04 LTS).</p>
<span id="more"></span>
<p>I found updating linux kernel to the latest version could be a solution:</p>
<ul>
<li><a href="https://www.reddit.com/r/Ubuntu/comments/1cfhnmd/ubuntu_2404_lts_qualcomm_ncm865_wifi_7_adapter/">https://www.reddit.com/r/Ubuntu/comments/1cfhnmd/ubuntu_2404_lts_qualcomm_ncm865_wifi_7_adapter/</a></li>
<li><a href="https://askubuntu.com/questions/1513315/issue-with-wireless-network-connection-on-ubuntu-24-04-lts">https://askubuntu.com/questions/1513315/issue-with-wireless-network-connection-on-ubuntu-24-04-lts</a></li>
</ul>
<h2 id="Update-nvidia-driver"><a href="#Update-nvidia-driver" class="headerlink" title="Update nvidia driver"></a>Update nvidia driver</h2><p>My first attempt to update the kernel failed.<br>This is because I am using an old version (535) of nvidia driver. (See: <a href="https://wiki.ubuntu.com/Kernel/MainlineBuilds">https://wiki.ubuntu.com/Kernel/MainlineBuilds</a>)</p>
<blockquote>
<p>First, if one is using select proprietary or out-of-tree modules<br>(e.g. bcmwl, fglrx, NVIDIA proprietary graphics drivers, VirtualBox, etc.)<br>unless there is an extra package available for the version you are testing,<br>you will need to uninstall the module first, in order to test the mainline kernel.<br>If you do not uninstall these modules first, then the upstream kernel may fail to install, or boot.</p>
</blockquote>
<p>After updating nvidia driver from 535 to the latest 550 in “Additional Drivers”,<br>I successfully updated the kernel as follows.</p>
<h2 id="Upgrade-Ubuntu-kernel-version"><a href="#Upgrade-Ubuntu-kernel-version" class="headerlink" title="Upgrade Ubuntu kernel version"></a>Upgrade Ubuntu kernel version</h2><p>Refer to: <a href="https://sypalo.com/how-to-upgrade-ubuntu">https://sypalo.com/how-to-upgrade-ubuntu</a></p>
<h3 id="Update-packages-list"><a href="#Update-packages-list" class="headerlink" title="Update packages list"></a>Update packages list</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure>
<h3 id="Upgrade-packages"><a href="#Upgrade-packages" class="headerlink" title="Upgrade packages"></a>Upgrade packages</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get upgrade</span><br></pre></td></tr></table></figure>
<h3 id="Run-full-upgrade"><a href="#Run-full-upgrade" class="headerlink" title="Run full upgrade"></a>Run full upgrade</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get dist-upgrade</span><br></pre></td></tr></table></figure>
<h3 id="Run-cleanup"><a href="#Run-cleanup" class="headerlink" title="Run cleanup"></a>Run cleanup</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt-get autoremove</span><br><span class="line">sudo apt-get clean</span><br></pre></td></tr></table></figure>
<h3 id="Change-current-directory-to-tmp-or-elsewhere-you-like"><a href="#Change-current-directory-to-tmp-or-elsewhere-you-like" class="headerlink" title="Change current directory to /tmp (or elsewhere you like)"></a>Change current directory to /tmp (or elsewhere you like)</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /tmp</span><br></pre></td></tr></table></figure>
<h3 id="Download-latest-stable-kernel-choose-from-https-kernel-ubuntu-com-mainline"><a href="#Download-latest-stable-kernel-choose-from-https-kernel-ubuntu-com-mainline" class="headerlink" title="Download latest stable kernel (choose from https://kernel.ubuntu.com/mainline/)"></a>Download latest stable kernel (choose from <a href="https://kernel.ubuntu.com/mainline/">https://kernel.ubuntu.com/mainline/</a>)</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">wget -c https://kernel.ubuntu.com/mainline/v6.10.10/amd64/linux-headers-6.10.10-061010_6.10.10-061010.202409121037_all.deb</span><br><span class="line">wget -c https://kernel.ubuntu.com/mainline/v6.10.10/amd64/linux-headers-6.10.10-061010-generic_6.10.10-061010.202409121037_amd64.deb</span><br><span class="line">wget -c https://kernel.ubuntu.com/mainline/v6.10.10/amd64/linux-image-unsigned-6.10.10-061010-generic_6.10.10-061010.202409121037_amd64.deb</span><br><span class="line">wget -c https://kernel.ubuntu.com/mainline/v6.10.10/amd64/linux-modules-6.10.10-061010-generic_6.10.10-061010.202409121037_amd64.deb</span><br></pre></td></tr></table></figure>
<h3 id="Install-latest-stable-kernel"><a href="#Install-latest-stable-kernel" class="headerlink" title="Install latest stable kernel"></a>Install latest stable kernel</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i *.deb</span><br></pre></td></tr></table></figure>
<h3 id="Reboot-system-after-latest-stable-kernel-upgrade"><a href="#Reboot-system-after-latest-stable-kernel-upgrade" class="headerlink" title="Reboot system after latest stable kernel upgrade"></a>Reboot system after latest stable kernel upgrade</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure>
<p>Finally, the Wi-Fi icon appears and I no longer need a LAN cable.</p>
]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
</search>
